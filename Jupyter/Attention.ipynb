{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minwenwen/.conda/envs/lxy/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import confusion_matrix,roc_curve,auc,recall_score, precision_score,accuracy_score,matthews_corrcoef, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import random_split\n",
    "from scipy.interpolate import make_interp_spline\n",
    "from scipy.signal import savgol_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "'NVIDIA GeForce RTX 3090 Ti'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(3407)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cpu')\n",
    "torch.cuda.get_device_name()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "cli_data_path = '/data/minwenwen/lixiaoyu/TCGA/data/BRCA/BRCA-300/BRCA_Clinical.csv'\n",
    "cna_data_path = '/data/minwenwen/lixiaoyu/TCGA/data/BRCA/BRCA-300/BRCA_CNA.csv'\n",
    "rna_data_path = '/data/minwenwen/lixiaoyu/TCGA/data/BRCA/BRCA-300/BRCA_RNA_V2_EXP.csv'\n",
    "mic_data_path = '/data/minwenwen/lixiaoyu/TCGA/data/BRCA/BRCA-300/BRCA_microbiome.csv'\n",
    "label_data_path = '/data/minwenwen/lixiaoyu/TCGA/data/BRCA/BRCA-300/BRCA_label.csv'\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_path, label_path):\n",
    "        super(MyDataset, self).__init__()\n",
    "        self.data = pd.read_csv(data_path, index_col=0)\n",
    "        self.label = pd.read_csv(label_path)\n",
    "        self.x_data = torch.tensor(self.data.values, dtype=torch.float32)\n",
    "        self.y_data = torch.tensor(self.label.values, dtype=torch.float32)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_data[idx], self.y_data[idx]\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "\n",
    "cli_data = MyDataset(cli_data_path, label_data_path)\n",
    "cna_data = MyDataset(cna_data_path, label_data_path)\n",
    "rna_data = MyDataset(rna_data_path, label_data_path)\n",
    "mic_data = MyDataset(mic_data_path, label_data_path)\n",
    "\n",
    "train_size = int(0.8 * len(cli_data))\n",
    "test_size = len(cli_data) - train_size\n",
    "cli_train_dataset, cli_test_dataset = random_split(cli_data, [train_size, test_size], generator=torch.Generator().manual_seed(3407))\n",
    "valid_size = int(0.25 * len(cli_train_dataset))\n",
    "train_size = len(cli_train_dataset) - valid_size\n",
    "cli_train_dataset, cli_valid_dataset = random_split(cli_train_dataset, [train_size, valid_size], generator=torch.Generator().manual_seed(3407))\n",
    "\n",
    "cli_train_dataloader = DataLoader(cli_train_dataset, batch_size=len(cli_train_dataset))\n",
    "cli_valid_dataloader = DataLoader(cli_valid_dataset, batch_size=len(cli_valid_dataset))\n",
    "cli_test_dataloader = DataLoader(cli_test_dataset, batch_size=len(cli_test_dataset))\n",
    "\n",
    "train_size = int(0.8 * len(cna_data))\n",
    "test_size = len(cna_data) - train_size\n",
    "cna_train_dataset, cna_test_dataset = random_split(cna_data, [train_size, test_size], generator=torch.Generator().manual_seed(3407))\n",
    "valid_size = int(0.25 * len(cna_train_dataset))\n",
    "train_size = len(cna_train_dataset) - valid_size\n",
    "cna_train_dataset, cna_valid_dataset = random_split(cna_train_dataset, [train_size, valid_size], generator=torch.Generator().manual_seed(3407))\n",
    "cna_train_dataloader = DataLoader(cna_train_dataset, batch_size=len(cna_train_dataset))\n",
    "cna_valid_dataloader = DataLoader(cna_valid_dataset, batch_size=len(cna_valid_dataset))\n",
    "cna_test_dataloader = DataLoader(cna_test_dataset, batch_size=len(cna_test_dataset))\n",
    "\n",
    "train_size = int(0.8 * len(rna_data))\n",
    "test_size = len(rna_data) - train_size\n",
    "rna_train_dataset, rna_test_dataset = random_split(rna_data, [train_size, test_size], generator=torch.Generator().manual_seed(3407))\n",
    "valid_size = int(0.25 * len(rna_train_dataset))\n",
    "train_size = len(rna_train_dataset) - valid_size\n",
    "rna_train_dataset, rna_valid_dataset = random_split(rna_train_dataset, [train_size, valid_size], generator=torch.Generator().manual_seed(3407))\n",
    "rna_train_dataloader = DataLoader(rna_train_dataset, batch_size=len(rna_train_dataset))\n",
    "rna_valid_dataloader = DataLoader(rna_valid_dataset, batch_size=len(rna_valid_dataset))\n",
    "rna_test_dataloader = DataLoader(rna_test_dataset, batch_size=len(rna_test_dataset))\n",
    "\n",
    "train_size = int(0.8 * len(mic_data))\n",
    "test_size = len(mic_data) - train_size\n",
    "mic_train_dataset, mic_test_dataset = random_split(mic_data, [train_size, test_size], generator=torch.Generator().manual_seed(3407))\n",
    "valid_size = int(0.25 * len(mic_train_dataset))\n",
    "train_size = len(mic_train_dataset) - valid_size\n",
    "mic_train_dataset, mic_valid_dataset = random_split(mic_train_dataset, [train_size, valid_size], generator=torch.Generator().manual_seed(3407))\n",
    "mic_train_dataloader = DataLoader(mic_train_dataset, batch_size=len(mic_train_dataset))\n",
    "mic_valid_dataloader = DataLoader(mic_valid_dataset, batch_size=len(mic_valid_dataset))\n",
    "mic_test_dataloader = DataLoader(mic_test_dataset, batch_size=len(mic_test_dataset))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "300"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pd.read_csv('/data/minwenwen/lixiaoyu/TCGA/data/BRCA/BRCA-300/BRCA_CNA.csv', index_col=0)\n",
    "x = torch.tensor(x.values, dtype=torch.float32)\n",
    "x.shape[1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#MultiHeadAttention\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, in_size, hidden_size=16):\n",
    "        super(Attention, self).__init__()\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Linear(in_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1, bias=False)\n",
    "        )\n",
    "    def forward(self, z):\n",
    "        w = self.project(z)\n",
    "        beta = torch.softmax(w, dim=1)\n",
    "        return (beta * z).sum(1), beta\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\" Scaled Dot-Product Attention \"\"\"\n",
    "    def __init__(self, scale):\n",
    "        super().__init__()\n",
    "        self.scale = scale\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        u = torch.matmul(q, k.transpose(-2, -1))\n",
    "        u = u / self.scale\n",
    "        if mask is not None:\n",
    "            u = u.masked_fill(mask, -np.inf)\n",
    "        attn = self.softmax(u)\n",
    "        output = torch.matmul(attn, v)\n",
    "        return attn, output\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_head, d_k_, d_v_, d_k, d_v, d_o):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.fc_q = nn.Linear(d_k_, n_head * d_k)\n",
    "        self.fc_k = nn.Linear(d_k_, n_head * d_k)\n",
    "        self.fc_v = nn.Linear(d_v_, n_head * d_v)\n",
    "        self.attention = ScaledDotProductAttention(scale=np.power(d_k, 0.5))\n",
    "        self.fc_o = nn.Linear(n_head * d_v, d_o)\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        n_head, d_q, d_k, d_v = self.n_head, self.d_k, self.d_k, self.d_v\n",
    "        n_q, d_q_ = q.size()\n",
    "        n_k, d_k_ = k.size()\n",
    "        n_v, d_v_ = v.size()\n",
    "        q = self.fc_q(q)\n",
    "        k = self.fc_k(k)\n",
    "        v = self.fc_v(v)\n",
    "        q = q.view(n_q, n_head, d_q).permute(1, 0, 2).contiguous().view(-1, n_q, d_q)\n",
    "        k = k.view(n_k, n_head, d_k).permute(1, 0, 2).contiguous().view(-1, n_k, d_k)\n",
    "        v = v.view(n_v, n_head, d_v).permute(1, 0, 2).contiguous().view(-1, n_v, d_v)\n",
    "        if mask is not None:\n",
    "            mask = mask.repeat(n_head, 1, 1)\n",
    "        attn, output = self.attention(q, k, v, mask=mask)\n",
    "        output = output.view(n_head, n_q, d_v).permute(0, 1, 2).contiguous().view(n_q, -1)\n",
    "        output = self.fc_o(output)\n",
    "        return attn, output\n",
    "\n",
    "class MultiSelfAttention(nn.Module):\n",
    "    def __init__(self, n_head, d_k, d_v, d_x, d_o):\n",
    "        super().__init__()\n",
    "        self.wq = nn.Parameter(torch.Tensor(d_x, d_k))\n",
    "        self.wk = nn.Parameter(torch.Tensor(d_x, d_k))\n",
    "        self.wv = nn.Parameter(torch.Tensor(d_x, d_v))\n",
    "        self.mha = MultiHeadAttention(n_head=n_head, d_k_=d_k, d_v_=d_v, d_k=d_k, d_v=d_v, d_o=d_o)\n",
    "        self.init_parameters()\n",
    "    def init_parameters(self):\n",
    "        for param in self.parameters():\n",
    "            stdv = 1. / np.power(param.size(-1), 0.5)\n",
    "            param.data.uniform_(-stdv, stdv)\n",
    "    def forward(self, x, mask=None):\n",
    "        q = torch.matmul(x, self.wq)\n",
    "        k = torch.matmul(x, self.wk)\n",
    "        v = torch.matmul(x, self.wv)\n",
    "        attn, output = self.mha(q, k, v, mask=mask)\n",
    "        return output\n",
    "\n",
    "class Sample(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Sample, self).__init__()\n",
    "    def forward(self, z_mean, z_log_var):\n",
    "        eps = torch.randn(z_mean.shape)\n",
    "        eps = eps.to(device)\n",
    "        std = torch.exp(z_log_var / 2)\n",
    "        out = z_mean + std*eps\n",
    "        return out\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(400, 256),\n",
    "            nn.Dropout(p=0.15, inplace=False),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.Dropout(p=0.15, inplace=False),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.Dropout(p=0.15, inplace=False),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.z_mean = nn.Linear(32, 6)\n",
    "        self.z_log_var = nn.Linear(32, 6)\n",
    "        self.sample = Sample()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        z_mean = self.z_mean(out)\n",
    "        z_log_var = self.z_log_var(out)\n",
    "        out = self.sample(z_mean, z_log_var)\n",
    "        return out, z_mean, z_log_var\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(6, 32),\n",
    "            nn.Dropout(p=0.15, inplace=False),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.Dropout(p=0.15, inplace=False),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64, 256),\n",
    "            nn.Dropout(p=0.15, inplace=False),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(256, 50),\n",
    "            nn.Dropout(p=0.15, inplace=False),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.model(z)\n",
    "        return out\n",
    "\n",
    "class Classfier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classfier, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(6, 100, bias = True), #input layer\n",
    "            nn.BatchNorm1d(100, eps = 1e-3),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=0.7, inplace=False),\n",
    "            nn.Linear(100, 2, bias = True), # 2 hidden layer\n",
    "            nn.Softmax(dim = 1)    #outpust layer\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "class MAVC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MAVC, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        self.classfier = Classfier()\n",
    "        self.attn1 = MultiSelfAttention(30, 64, 32, 22, 100)\n",
    "        self.attn2 = MultiSelfAttention(30, 64, 32, 300, 100)\n",
    "\n",
    "    def forward(self, clinical_input, cna_input, micro_input, rna_input):\n",
    "        cli_in = self.attn1(clinical_input)\n",
    "        cna_in = self.attn2(cna_input)\n",
    "        mic_in = self.attn2(micro_input)\n",
    "        rna_in = self.attn2(rna_input)\n",
    "        x = torch.cat((cli_in, cna_in, mic_in, rna_in), dim = 1)\n",
    "        x, mean, var = self.encoder(x)\n",
    "        x = self.classfier(x)\n",
    "        return x\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "model = MAVC()\n",
    "model.to(device)\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.RAdam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion.to(device)\n",
    "num_epoch = 100\n",
    "max_accuracy = 0\n",
    "vae_acc = 0\n",
    "vae_pre = 0\n",
    "vae_sen = 0\n",
    "vae_f1s = 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                             \r"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(num_epoch), leave=False):\n",
    "    model.train()\n",
    "    for i, data in enumerate(zip(cli_train_dataloader,cna_train_dataloader,mic_train_dataloader,rna_train_dataloader)):\n",
    "        clinical_data, cna_data, mic_data, rna_data, y_t = data[0][0], data[1][0], data[2][0], data[3][0], data[0][1]\n",
    "        clinical_data = clinical_data.to(device)\n",
    "        cna_data = cna_data.to(device)\n",
    "        mic_data = mic_data.to(device)\n",
    "        rna_data = rna_data.to(device)\n",
    "        y_t = y_t.to(device)\n",
    "        y_pred = model(clinical_data, cna_data, mic_data, rna_data)\n",
    "        y_t = y_t.squeeze(dim = 1)\n",
    "        loss = criterion(y_pred, y_t.long())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    total_test_loss = 0\n",
    "    total_accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(zip(cli_valid_dataloader,cna_valid_dataloader,mic_valid_dataloader,rna_valid_dataloader)):\n",
    "            clinical_data, cna_data, mic_data, rna_data, y_t = data[0][0], data[1][0], data[2][0], data[3][0], data[0][1]\n",
    "            clinical_data = clinical_data.to(device)\n",
    "            cna_data = cna_data.to(device)\n",
    "            mic_data = mic_data.to(device)\n",
    "            rna_data = rna_data.to(device)\n",
    "            y_t = y_t.to(device)\n",
    "            y_pred = model(clinical_data, cna_data, mic_data, rna_data)\n",
    "            y_t = y_t.squeeze(dim = 1)\n",
    "            loss = criterion(y_pred, y_t.long())\n",
    "            total_test_loss = total_test_loss + loss.item()\n",
    "            accuracy = (y_pred.argmax(1) == y_t).sum()\n",
    "            total_accuracy = total_accuracy + accuracy\n",
    "    if((total_accuracy/len(cna_valid_dataset)) > max_accuracy):\n",
    "        max_accuracy = total_accuracy/len(cna_valid_dataset)\n",
    "        torch.save(model, \"/data/minwenwen/lixiaoyu/TCGA/Model/MAVC_Best.pth\")\n",
    "\n",
    "model = torch.load('/data/minwenwen/lixiaoyu/TCGA/Model/MAVC_Best.pth')\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(zip(cli_test_dataloader,cna_test_dataloader,mic_test_dataloader,rna_test_dataloader)):\n",
    "        clinical_data, cna_data, mic_data, rna_data, mavc_y_t = data[0][0],data[1][0],data[2][0],data[3][0],data[0][1]\n",
    "        clinical_data = clinical_data.to(device)\n",
    "        cna_data = cna_data.to(device)\n",
    "        mic_data = mic_data.to(device)\n",
    "        rna_data = rna_data.to(device)\n",
    "        mavc_y_t = y_t.to(device)\n",
    "        mavc_y_pred = model(clinical_data, cna_data, mic_data, rna_data)\n",
    "        # mavc_y_t = mavc_y_t.squeeze(dim = 1)\n",
    "\n",
    "        mavc_y_t = mavc_y_t.detach().cpu().numpy()\n",
    "        mavc_y_pred = mavc_y_pred.detach().cpu().numpy()\n",
    "        mavc_acc = accuracy_score(mavc_y_t, mavc_y_pred.argmax(1))\n",
    "        mavc_pre = precision_score(mavc_y_t, mavc_y_pred.argmin(1))\n",
    "        mavc_sen = recall_score(mavc_y_t, mavc_y_pred.argmin(1))\n",
    "        mavc_f1s = f1_score(mavc_y_t, mavc_y_pred.argmin(1))\n",
    "        mavc_y_pred = np.amax(mavc_y_pred, axis=1)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "model = torch.load('/data/minwenwen/lixiaoyu/TCGA/Model/MAVC_Best.pth')\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(zip(cli_test_dataloader,cna_test_dataloader,mic_test_dataloader,rna_test_dataloader)):\n",
    "        clinical_data, cna_data, mic_data, rna_data, mavc_y_t = data[0][0],data[1][0],data[2][0],data[3][0],data[0][1]\n",
    "        clinical_data = clinical_data.to(device)\n",
    "        cna_data = cna_data.to(device)\n",
    "        mic_data = mic_data.to(device)\n",
    "        rna_data = rna_data.to(device)\n",
    "        mavc_y_t = y_t.to(device)\n",
    "        mavc_y_pred = model(clinical_data, cna_data, mic_data, rna_data)\n",
    "        # mavc_y_t = mavc_y_t.squeeze(dim = 1)\n",
    "\n",
    "        mavc_y_t = mavc_y_t.detach().cpu().numpy()\n",
    "        mavc_y_pred = mavc_y_pred.detach().cpu().numpy()\n",
    "        mavc_acc = accuracy_score(mavc_y_t, mavc_y_pred.argmax(1))\n",
    "        mavc_pre = precision_score(mavc_y_t, mavc_y_pred.argmin(1))\n",
    "        mavc_sen = recall_score(mavc_y_t, mavc_y_pred.argmin(1))\n",
    "        mavc_f1s = f1_score(mavc_y_t, mavc_y_pred.argmin(1))\n",
    "        mavc_y_pred = np.amax(mavc_y_pred, axis=1)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [9], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m mavc_fpr, mavc_tpr, mavc_threshold \u001B[38;5;241m=\u001B[39m \u001B[43mroc_curve\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmavc_y_t\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmavc_y_pred\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m mavc_roc_auc \u001B[38;5;241m=\u001B[39m auc(mavc_fpr, mavc_tpr)\n",
      "File \u001B[0;32m~/.conda/envs/lxy/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:981\u001B[0m, in \u001B[0;36mroc_curve\u001B[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001B[0m\n\u001B[1;32m    892\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mroc_curve\u001B[39m(\n\u001B[1;32m    893\u001B[0m     y_true, y_score, \u001B[38;5;241m*\u001B[39m, pos_label\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, sample_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, drop_intermediate\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    894\u001B[0m ):\n\u001B[1;32m    895\u001B[0m     \u001B[38;5;124;03m\"\"\"Compute Receiver operating characteristic (ROC).\u001B[39;00m\n\u001B[1;32m    896\u001B[0m \n\u001B[1;32m    897\u001B[0m \u001B[38;5;124;03m    Note: this implementation is restricted to the binary classification task.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    979\u001B[0m \n\u001B[1;32m    980\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 981\u001B[0m     fps, tps, thresholds \u001B[38;5;241m=\u001B[39m \u001B[43m_binary_clf_curve\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    982\u001B[0m \u001B[43m        \u001B[49m\u001B[43my_true\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_score\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpos_label\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpos_label\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\n\u001B[1;32m    983\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    985\u001B[0m     \u001B[38;5;66;03m# Attempt to drop thresholds corresponding to points in between and\u001B[39;00m\n\u001B[1;32m    986\u001B[0m     \u001B[38;5;66;03m# collinear with other points. These are always suboptimal and do not\u001B[39;00m\n\u001B[1;32m    987\u001B[0m     \u001B[38;5;66;03m# appear on a plotted ROC curve (and thus do not affect the AUC).\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    992\u001B[0m     \u001B[38;5;66;03m# but does not drop more complicated cases like fps = [1, 3, 7],\u001B[39;00m\n\u001B[1;32m    993\u001B[0m     \u001B[38;5;66;03m# tps = [1, 2, 4]; there is no harm in keeping too many thresholds.\u001B[39;00m\n\u001B[1;32m    994\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m drop_intermediate \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(fps) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m2\u001B[39m:\n",
      "File \u001B[0;32m~/.conda/envs/lxy/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:746\u001B[0m, in \u001B[0;36m_binary_clf_curve\u001B[0;34m(y_true, y_score, pos_label, sample_weight)\u001B[0m\n\u001B[1;32m    744\u001B[0m y_score \u001B[38;5;241m=\u001B[39m column_or_1d(y_score)\n\u001B[1;32m    745\u001B[0m assert_all_finite(y_true)\n\u001B[0;32m--> 746\u001B[0m \u001B[43massert_all_finite\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_score\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    748\u001B[0m \u001B[38;5;66;03m# Filter out zero-weighted samples, as they should not impact the result\u001B[39;00m\n\u001B[1;32m    749\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m sample_weight \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/.conda/envs/lxy/lib/python3.8/site-packages/sklearn/utils/validation.py:180\u001B[0m, in \u001B[0;36massert_all_finite\u001B[0;34m(X, allow_nan, estimator_name, input_name)\u001B[0m\n\u001B[1;32m    154\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21massert_all_finite\u001B[39m(\n\u001B[1;32m    155\u001B[0m     X,\n\u001B[1;32m    156\u001B[0m     \u001B[38;5;241m*\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    159\u001B[0m     input_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    160\u001B[0m ):\n\u001B[1;32m    161\u001B[0m     \u001B[38;5;124;03m\"\"\"Throw a ValueError if X contains NaN or infinity.\u001B[39;00m\n\u001B[1;32m    162\u001B[0m \n\u001B[1;32m    163\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    178\u001B[0m \u001B[38;5;124;03m        documentation.\u001B[39;00m\n\u001B[1;32m    179\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 180\u001B[0m     \u001B[43m_assert_all_finite\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    181\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43msp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43missparse\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    182\u001B[0m \u001B[43m        \u001B[49m\u001B[43mallow_nan\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mallow_nan\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    183\u001B[0m \u001B[43m        \u001B[49m\u001B[43mestimator_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mestimator_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    184\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    185\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/lxy/lib/python3.8/site-packages/sklearn/utils/validation.py:146\u001B[0m, in \u001B[0;36m_assert_all_finite\u001B[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001B[0m\n\u001B[1;32m    124\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    125\u001B[0m             \u001B[38;5;129;01mnot\u001B[39;00m allow_nan\n\u001B[1;32m    126\u001B[0m             \u001B[38;5;129;01mand\u001B[39;00m estimator_name\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    130\u001B[0m             \u001B[38;5;66;03m# Improve the error message on how to handle missing values in\u001B[39;00m\n\u001B[1;32m    131\u001B[0m             \u001B[38;5;66;03m# scikit-learn.\u001B[39;00m\n\u001B[1;32m    132\u001B[0m             msg_err \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    133\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mestimator_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m does not accept missing values\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    134\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    144\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m#estimators-that-handle-nan-values\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    145\u001B[0m             )\n\u001B[0;32m--> 146\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg_err)\n\u001B[1;32m    148\u001B[0m \u001B[38;5;66;03m# for object dtype data, we only check for NaNs (GH-13254)\u001B[39;00m\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m X\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m==\u001B[39m np\u001B[38;5;241m.\u001B[39mdtype(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mobject\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m allow_nan:\n",
      "\u001B[0;31mValueError\u001B[0m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "mavc_fpr, mavc_tpr, mavc_threshold = roc_curve(mavc_y_t, mavc_y_pred)\n",
    "mavc_roc_auc = auc(mavc_fpr, mavc_tpr)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "lxy",
   "language": "python",
   "display_name": "lxy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
